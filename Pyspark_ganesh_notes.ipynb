{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a375083-779f-4012-909d-418334ee6b64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e428bcde-2527-49a2-baa2-caee1b71fe3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\n| id|location_id|           address_1|address_2|        city|state_province|postal_code|country|\n+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\n|  1|          1|2600 Middlefield ...|     null|Redwood City|            CA|      94063|     US|\n|  2|          2|    24 Second Avenue|     null|   San Mateo|            CA|      94401|     US|\n|  3|          3|    24 Second Avenue|     null|   San Mateo|            CA|      94403|     US|\n|  4|          4|    24 Second Avenue|     null|   San Mateo|            CA|      94401|     US|\n+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Exercise 1\").getOrCreate()\n",
    "# Load data from CSV into a DataFrame\n",
    "data_df = spark.read.csv(\"dbfs:/FileStore/addresses.csv\", header=True, inferSchema=True)\n",
    "# Show the first few rows of the DataFrame\n",
    "data_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a079921-e99d-4e6d-a3ce-8f141a64c07d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ff1483-9221-4370-9870-2cc074620c46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- location_id: integer (nullable = true)\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state_province: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Display the schema of the DataFrame\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de756694-fbfd-4e9a-b705-cbebe45948d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a3663f-f443-4227-9cc7-1c3a88127efc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\n| id|location_id|           address_1|address_2|        city|state_province|postal_code|country|\n+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\n| 11|         11| 2140 Euclid Avenue.|     null|Redwood City|            CA|      94061|     US|\n| 12|         12|1044 Middlefield ...|2nd Floor|Redwood City|            CA|      94063|     US|\n| 13|         13| 399 Marine Parkway.|     null|Redwood City|            CA|      94065|     US|\n| 14|         14|  660 Veterans Blvd.|     null|Redwood City|            CA|      94063|     US|\n+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Filter the DataFrame\n",
    "filtered_df = data_df.filter(col(\"id\") > 10)\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2c76dbb-dc20-4ce7-ab2e-ec1f8b926671",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca53b882-edcf-4df4-9f13-a1f63bfe0cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\n| id|location_id|           address_1|address_2|        city|state_province|postal_code|country|\n+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\n|  2|          1|2600 Middlefield ...|     null|Redwood City|            CA|      94063|     US|\n|  4|          2|    24 Second Avenue|     null|   San Mateo|            CA|      94401|     US|\n|  6|          3|    24 Second Avenue|     null|   San Mateo|            CA|      94403|     US|\n|  8|          4|    24 Second Avenue|     null|   San Mateo|            CA|      94401|     US|\n+---+-----------+--------------------+---------+------------+--------------+-----------+-------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Create the new column\n",
    "transformed_df = data_df.withColumn(\"id\", expr(\"id * 2\"))\n",
    "# Show the DataFrame with the new column\n",
    "transformed_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fd1064-9237-4575-b021-cd64ee043f96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5.Daat Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b553c0-939f-456e-b9a4-e4f835fd871b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average id: 11.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# Calculate the average age\n",
    "avg_id = data_df.select(avg(\"id\")).first()[0]\n",
    "print(\"Average id:\", avg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00de7439-5aa6-4dae-b660-2e79d6379f42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6.Data Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "745f0e58-11f5-4ff4-b8b7-6755be9ff707",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by \"gender\" and calculate the average age\n",
    "grouped_df = data_df.groupBy(\"gender\").agg(avg(\"age\").alias(\"average_age\"))\n",
    "# Show the grouped DataFrame\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb98f5f-22f6-44f1-b87c-4919b3a71826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7.Joining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91544b93-3d98-4f13-b8b8-776c3427e4eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load employees data from CSV into DataFrame\n",
    "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load departments data from CSV into DataFrame\n",
    "departments_df = spark.read.csv(\"departments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Join the DataFrames on \"department_id\"\n",
    "joined_df = employees_df.join(departments_df, \"department_id\")\n",
    "\n",
    "# Show the joined DataFrame\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eb6b6e1-6533-449a-bd27-18396335eddc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 8.Data Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ba75db-120e-4509-a467-4046dccbdae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplicate the DataFrame\n",
    "deduplicated_df = joined_df.dropDuplicates()\n",
    "# Show the deduplicated DataFrame\n",
    "deduplicated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe6b6f0-50e7-4f0f-b655-c4ed71ff5d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 9.Data Imputation\n",
    "* Replace the missing values in the \"salary\" column with the average salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ea508c9-a70e-41c8-a691-4ce08eaf479a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average salary\n",
    "avg_salary = joined_df.select(avg(\"salary\")).first()[0]\n",
    "\n",
    "# Replace missing values with average salary\n",
    "imputed_df = joined_df.na.fill(avg_salary, subset=[\"salary\"])\n",
    "\n",
    "# Show the DataFrame with imputed values\n",
    "imputed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba4d3fd-0709-4025-bafc-1342d5695ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 10. Data Aggregation with GroupBy\n",
    "* Group the DataFrame by the \"department_name\" column and calculate the total salary for each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "703d71ab-c2f4-45ff-a3ea-426daf2f85c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by \"department_name\" and calculate the total salary\n",
    "salary_by_department_df = joined_df.groupBy(\"department_name\").agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "\n",
    "# Show the DataFrame with total salary for each department\n",
    "salary_by_department_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02141f5e-62ab-4cb4-a23c-bb42082cb96a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 11.Data sorting\n",
    "* sort the DataFrame based on the \"age\" column in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb698a3-0995-4b5b-98c5-92ad209ddbe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort the DataFrame by \"age\" in ascending order\n",
    "sorted_df = joined_df.orderBy(\"age\")\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3711f6ca-023c-4b0c-a256-9038429a64e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 12.Data Joining with Different Column Names\n",
    "* Load two CSV files \"employees.csv\" and \"departments.csv\" into separate DataFrames. Join the DataFrames on \"dept_id\" from the \"employees\" DataFrame and \"id\" from the \"departments\" DataFrame to get a single DataFrame containing both employee and department information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ea9443-d30e-4e18-aec1-a6f724328865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load employees data from CSV into DataFrame with custom column names\n",
    "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True).withColumnRenamed(\"dept_id\", \"department_id\")\n",
    "\n",
    "# Load departments data from CSV into DataFrame\n",
    "departments_df = spark.read.csv(\"departments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Join the DataFrames on different column names\n",
    "joined_df = employees_df.join(departments_df, employees_df[\"department_id\"] == departments_df[\"id\"])\n",
    "\n",
    "# Show the joined DataFrame\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3ce567-28e1-4d8a-a1df-e9b829eb972b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 13.Data Repartitioning\n",
    "* Repartition the DataFrame into 5 partitions for better parallelism during processing.\n",
    "* The provided code will repartition the DataFrame into 5 partitions, allowing for better parallelism during processing, and display the repartitioned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610800a-ef2f-4b5b-aed5-8f008be7fe17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition the DataFrame into 5 partitions\n",
    "repartitioned_df = joined_df.repartition(5)\n",
    "\n",
    "# Show the repartitioned DataFrame\n",
    "repartitioned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b85dbe-330f-4de4-a7e7-4f23ae3ba314",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 14.Data Window Functions\n",
    "* Calculate the rank of employees based on their salaries within each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c074560-5d44-4ffb-a05c-f8c548199a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"department_name\").orderBy(joined_df[\"salary\"].desc())\n",
    "\n",
    "# Calculate the rank of employees based on salaries within each department\n",
    "ranked_df = joined_df.withColumn(\"salary_rank\", rank().over(window_spec))\n",
    "\n",
    "# Show the DataFrame with salary ranks\n",
    "ranked_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1862f64-9e46-4909-bc0f-7a5be3478520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 15.Data Filtering with SQL\n",
    "* Filter the DataFrame to include only employees whose salary is above 50000 using SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f1f9b9f-efb3-468e-9da8-f28c1066cd69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary SQL table\n",
    "joined_df.createOrReplaceTempView(\"employee_data\")\n",
    "\n",
    "# Perform SQL filtering\n",
    "filtered_df = spark.sql(\"SELECT * FROM employee_data WHERE salary > 50000\")\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759a3881-96e8-4da9-bb2e-7aaad43e205c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 16.Data Pivot\n",
    "* Pivot the DataFrame to transform rows into columns based on the \"gender\" column, and calculate the average salary for each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29624b78-1d41-4c2a-98e1-498c81b8cd70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivot the DataFrame\n",
    "pivot_df = joined_df.groupBy(\"department_name\").pivot(\"gender\").avg(\"salary\")\n",
    "\n",
    "# Show the pivoted DataFrame\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb1fa322-b36a-4f5b-81a4-373face44473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##17.Data Joins - Left Outer Join\n",
    "* Perform a left outer join between the \"employees\" DataFrame and the \"departments\" DataFrame on the \"department_id\" column, and display the combined DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0210eb2-7999-475c-a440-e7987a256a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform the left outer join\n",
    "left_outer_join_df = employees_df.join(departments_df, on=\"department_id\", how=\"left\")\n",
    "# Show the combined DataFrame\n",
    "left_outer_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "021e439f-c72d-4292-940f-e544e5e21dec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 18.Data Coalesce\n",
    "* Create a new DataFrame with the \"first_name\" and \"last_name\" columns coalesced into a single \"full_name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d3444c-6cd1-47a0-86e3-258c4da2b606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the new DataFrame with coalesced column\n",
    "coalesced_df = joined_df.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "\n",
    "# Show the DataFrame with the new \"full_name\" column\n",
    "coalesced_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "998aeef7-5782-4946-845f-afd2ed299b26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 19.Data Aggregation - Count and GroupBy\n",
    "* Count the number of employees in each department and display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e7c4063-a6df-41b3-9fa3-e58f9f9bba0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of employees in each department\n",
    "employee_count_df = joined_df.groupBy(\"department_name\").count()\n",
    "\n",
    "# Show the DataFrame with employee counts\n",
    "employee_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb79b5d7-6eda-4f3e-afae-43396872c38d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 20.Data Sampling\n",
    "* Randomly sample 10% of the data from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e18857-8a2c-4676-91e2-1b26fb8a76e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Randomly sample 10% of the data\n",
    "sampled_df = joined_df.sample(0.1)\n",
    "\n",
    "# Show the sampled DataFrame\n",
    "sampled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f8dc6c9-edb7-46fd-98a3-224f554f4c71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##21.Data Window Functions - Lag\n",
    "* Create a new column \"lag_salary\" that contains the previous salary of each employee based on the \"salary\" column, within each department.\n",
    "\n",
    "*Create a new column \"lag_salary\" that contains the previous salary of each employee based on the \"salary\" column, within each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c28d2cb-5eb5-4072-b1dd-676685418dca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"department_name\").orderBy(\"employee_id\")\n",
    "\n",
    "# Create the new column with lagged salary\n",
    "lagged_df = joined_df.withColumn(\"lag_salary\", lag(\"salary\").over(window_spec))\n",
    "\n",
    "# Show the DataFrame with the new column\n",
    "lagged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "607eb804-d014-46bf-8b08-21251d698167",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 22.Data UDF - User-Defined Function\n",
    "* Define a user-defined function (UDF) to categorize employees based on their salary. The function should return \"High\" for salaries greater than 75000, \"Medium\" for salaries between 50000 and 75000, and \"Low\" for salaries below 50000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27bf87b5-6a0b-4473-a920-fa88303da155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the UDF function\n",
    "def categorize_salary(salary):\n",
    "    if salary > 75000:\n",
    "        return \"High\"\n",
    "    elif 50000 <= salary <= 75000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# Register the UDF\n",
    "categorize_salary_udf = udf(categorize_salary, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column\n",
    "categorized_df = joined_df.withColumn(\"salary_category\", categorize_salary_udf(\"salary\"))\n",
    "\n",
    "# Show the DataFrame with the new column\n",
    "categorized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ded070aa-0e0c-47b5-b50f-89594902ebf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 23.Data Filtering - Multiple Conditions\n",
    "* Filter the DataFrame to include only male employees who have a salary above 60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fafb592-b196-4e18-ace5-4fd9ae9240b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame with multiple conditions\n",
    "filtered_df = joined_df.filter((joined_df[\"gender\"] == \"Male\") & (joined_df[\"salary\"] > 60000))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9efa4c1a-b2e0-492d-874c-ab5a057d6ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 24.Data Aggregation - Maximum Salary\n",
    "* Calculate the maximum salary from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53414d1e-667e-4900-b673-88c2be42de3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the maximum salary\n",
    "max_salary = joined_df.agg({\"salary\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Display the maximum salary\n",
    "print(\"Maximum Salary:\", max_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe242dd-8e5f-4544-9fe0-2f9608441e37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 25.Data Sampling - Stratified Sampling\n",
    "* Perform stratified sampling on the DataFrame to get a sample of 20% for each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4293dbe6-b5b6-48f8-98f8-eeeb9350e6a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform stratified sampling on the DataFrame\n",
    "stratified_sample_df = joined_df.sampleBy(\"department_name\", fractions={\"HR\": 0.2, \"Engineering\": 0.2, \"Marketing\": 0.2})\n",
    "\n",
    "# Show the stratified sampled DataFrame\n",
    "stratified_sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "877d152f-4ef2-4b95-9704-be7d7e5a6a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_ganesh_notes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
